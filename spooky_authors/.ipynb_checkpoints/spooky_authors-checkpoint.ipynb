{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "I've read an [article](http://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html) about LDA (\"Latent Dirichlet Allocation\"). I'm sure I haven't understand a lot of it but I want to try it for the Spooky Authors dataset.\n",
    "\n",
    "## What is LDA in short?\n",
    "\n",
    "*Latent Dirichlet allocation (LDA) is a topic model that generates topics based on word frequency from a set of documents. LDA is particularly useful for finding reasonably accurate mixtures of topics within a given document set.*\n",
    "\n",
    "**It's used to find what is a text about.** That's pretty much the takeaway from the article above. I want to try it because the authors may have some common themes in their texts that they write about. Also, we've found that they have lived in different periods of time so this may also affect their topics.\n",
    "\n",
    "## Steps to use the model\n",
    "\n",
    "1. We need to split the text into a list of elements. That's the so-called `Tokenization`.\n",
    "2. Once we have the tokens we need to clean them. We will remove the stop-words as Lucho did last time.\n",
    "3. Stemming - removes the endings of the words and reserves only their roots (*rougly*)\n",
    "\n",
    "## Let's dive in and see what will happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 2) (8392, 1) (8392, 3)\n",
      "{'author'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"data/train.zip\", index_col=['id'])\n",
    "test = pd.read_csv(\"data/test.zip\", index_col=['id'])\n",
    "sample_submission = pd.read_csv(\"data/sample_submission.zip\", index_col=['id'])\n",
    "\n",
    "print(train.shape, test.shape, sample_submission.shape)\n",
    "print(set(train.columns) - set(test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's tokenize the text first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'process', 'however', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the', 'dimensions', 'of', 'my', 'dungeon', 'as', 'I', 'might', 'make', 'its', 'circuit', 'and', 'return', 'to', 'the', 'point', 'whence', 'I', 'set', 'out', 'without', 'being', 'aware', 'of', 'the', 'fact', 'so', 'perfectly', 'uniform', 'seemed', 'the', 'wall']\n",
      "[\"I'm\", 'Martin', \"It's\", 'going', 'to', 'be', 'awesome', 'data', 'exploration']\n",
      "[\"Let's\", 'try', 'with', 'some', 'words', 'with', 'dashes', 'like', 'so-called']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"'?\\b[0-9A-Za-z'-]+\\b'?\")  # matches words and words with apostrophes\n",
    "# test the regex first as it wasn't a top match in SO and I've changed it a little bit...\n",
    "tokens1 = tokenizer.tokenize(train.iloc[0]['text'])\n",
    "tokens2 = tokenizer.tokenize(\"I'm Martin. It's going to be awesome data exploration\")\n",
    "tokens3 = tokenizer.tokenize(\"Let's try with some words with dashes like so-called\")\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "print(tokens3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The regex looks working. Let's change our `text` column to `tokens`\n",
    "\n",
    "I will make a function that takes a `DataFrame` and returns a new one with the mutated `text` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id26305</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[This, process, however, afforded, me, no, mea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17569</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[It, never, once, occurred, to, me, that, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id11008</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[In, his, left, hand, was, a, gold, snuff, box...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27763</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[How, lovely, is, spring, As, we, looked, from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id12958</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[Finding, nothing, else, not, even, gold, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id22965</th>\n",
       "      <td>A youth passed in solitude, my best years spen...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[A, youth, passed, in, solitude, my, best, yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id09674</th>\n",
       "      <td>The astronomer, perhaps, at this point, took r...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[The, astronomer, perhaps, at, this, point, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id13515</th>\n",
       "      <td>The surcingle hung in ribands from my body.</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[The, surcingle, hung, in, ribands, from, my, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id19322</th>\n",
       "      <td>I knew that you could not say to yourself 'ste...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[I, knew, that, you, could, not, say, to, your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id00912</th>\n",
       "      <td>I confess that neither the structure of langua...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[I, confess, that, neither, the, structure, of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text author  \\\n",
       "id                                                                  \n",
       "id26305  This process, however, afforded me no means of...    EAP   \n",
       "id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "id22965  A youth passed in solitude, my best years spen...    MWS   \n",
       "id09674  The astronomer, perhaps, at this point, took r...    EAP   \n",
       "id13515        The surcingle hung in ribands from my body.    EAP   \n",
       "id19322  I knew that you could not say to yourself 'ste...    EAP   \n",
       "id00912  I confess that neither the structure of langua...    MWS   \n",
       "\n",
       "                                                    tokens  \n",
       "id                                                          \n",
       "id26305  [This, process, however, afforded, me, no, mea...  \n",
       "id17569  [It, never, once, occurred, to, me, that, the,...  \n",
       "id11008  [In, his, left, hand, was, a, gold, snuff, box...  \n",
       "id27763  [How, lovely, is, spring, As, we, looked, from...  \n",
       "id12958  [Finding, nothing, else, not, even, gold, the,...  \n",
       "id22965  [A, youth, passed, in, solitude, my, best, yea...  \n",
       "id09674  [The, astronomer, perhaps, at, this, point, to...  \n",
       "id13515  [The, surcingle, hung, in, ribands, from, my, ...  \n",
       "id19322  [I, knew, that, you, could, not, say, to, your...  \n",
       "id00912  [I, confess, that, neither, the, structure, of...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_df_for_LAD(df):\n",
    "    def tokenize(text):\n",
    "        return tokenizer.tokenize(text)\n",
    "    \n",
    "    new_df = df.copy()\n",
    "    \n",
    "    new_df['tokens'] = new_df.apply(lambda r: tokenize(r['text']), axis=1)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# test the new method\n",
    "new_train = prepare_df_for_LAD(df=train)\n",
    "new_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seems good. Next step - remove the stop-words\n",
    "\n",
    "I will use [this Python library](https://pypi.python.org/pypi/stop-words) and update our `prepare_df_for_LAD` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id26305</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[This, process, however, afforded, means, asce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17569</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[It, never, occurred, fumbling, might, mere, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id11008</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[In, left, hand, gold, snuff, box, capered, hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27763</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[How, lovely, spring, As, looked, Windsor, Ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id12958</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[Finding, nothing, else, even, gold, Superinte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id22965</th>\n",
       "      <td>A youth passed in solitude, my best years spen...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[A, youth, passed, solitude, best, years, spen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id09674</th>\n",
       "      <td>The astronomer, perhaps, at this point, took r...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[The, astronomer, perhaps, point, took, refuge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id13515</th>\n",
       "      <td>The surcingle hung in ribands from my body.</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[The, surcingle, hung, ribands, body]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id19322</th>\n",
       "      <td>I knew that you could not say to yourself 'ste...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[I, knew, say, 'stereotomy', without, brought,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id00912</th>\n",
       "      <td>I confess that neither the structure of langua...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[I, confess, neither, structure, languages, co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text author  \\\n",
       "id                                                                  \n",
       "id26305  This process, however, afforded me no means of...    EAP   \n",
       "id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "id22965  A youth passed in solitude, my best years spen...    MWS   \n",
       "id09674  The astronomer, perhaps, at this point, took r...    EAP   \n",
       "id13515        The surcingle hung in ribands from my body.    EAP   \n",
       "id19322  I knew that you could not say to yourself 'ste...    EAP   \n",
       "id00912  I confess that neither the structure of langua...    MWS   \n",
       "\n",
       "                                                    tokens  \n",
       "id                                                          \n",
       "id26305  [This, process, however, afforded, means, asce...  \n",
       "id17569  [It, never, occurred, fumbling, might, mere, m...  \n",
       "id11008  [In, left, hand, gold, snuff, box, capered, hi...  \n",
       "id27763  [How, lovely, spring, As, looked, Windsor, Ter...  \n",
       "id12958  [Finding, nothing, else, even, gold, Superinte...  \n",
       "id22965  [A, youth, passed, solitude, best, years, spen...  \n",
       "id09674  [The, astronomer, perhaps, point, took, refuge...  \n",
       "id13515              [The, surcingle, hung, ribands, body]  \n",
       "id19322  [I, knew, say, 'stereotomy', without, brought,...  \n",
       "id00912  [I, confess, neither, structure, languages, co...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "\n",
    "def prepare_df_for_LAD(df):\n",
    "    def remove_stop_words(tokens):\n",
    "        english_stop_words = get_stop_words('en')\n",
    "\n",
    "        return [token for token in tokens if token not in english_stop_words]\n",
    "\n",
    "    def tokenize(text):\n",
    "        intiial_tokens = tokenizer.tokenize(text)\n",
    "        no_stop_words_tokens = remove_stop_words(intiial_tokens)\n",
    "        \n",
    "        return no_stop_words_tokens\n",
    "\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    new_df['tokens'] = new_df.apply(lambda r: tokenize(r['text']), axis=1)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "\n",
    "new_train = prepare_df_for_LAD(df=train)\n",
    "new_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are almost ready\n",
    "\n",
    "What's left? As I told you in the beginning we need to use `Stemming`. As we used the same technique during [the lecture](http://fmi.machine-learning.bg/lectures/08-spooky-author-identification), I changed my mind!\n",
    "\n",
    "Let's use `Lemmatization`, instead! It's almost the same as the `Stemming` technique but *on steroids*. It can make difference from *-ing* nouns and verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/martin056/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "process\n",
      "however\n",
      "afforded\n",
      "mean\n",
      "ascertaining\n",
      "dimension\n",
      "dungeon\n",
      "I\n",
      "might\n",
      "make\n",
      "circuit\n",
      "return\n",
      "point\n",
      "whence\n",
      "I\n",
      "set\n",
      "without\n",
      "aware\n",
      "fact\n",
      "perfectly\n",
      "uniform\n",
      "seemed\n",
      "wall\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# I will try it first, as I'm using it for first time\n",
    "for token in new_train.iloc[0]['tokens']:\n",
    "    print(lemmatizer.lemmatize(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cool. We headed into some problems\n",
    "\n",
    "1. We need to `lowercase()` the tokens\n",
    "2. We need to make the differnce if the word is a verb or a noun by ourselves.\n",
    "\n",
    "Well, it's not that \"by ourselves\". I've found something called `pos_tag` in **nltk**. Let's explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/martin056/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('process', 'NN'),\n",
       " ('however', 'RB'),\n",
       " ('afforded', 'VBD'),\n",
       " ('means', 'NNS'),\n",
       " ('ascertaining', 'VBG'),\n",
       " ('dimensions', 'NNS'),\n",
       " ('dungeon', 'VBP'),\n",
       " ('I', 'PRP'),\n",
       " ('might', 'MD'),\n",
       " ('make', 'VB'),\n",
       " ('circuit', 'NN'),\n",
       " ('return', 'NN'),\n",
       " ('point', 'NN'),\n",
       " ('whence', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('set', 'VBP'),\n",
       " ('without', 'IN'),\n",
       " ('aware', 'JJ'),\n",
       " ('fact', 'NN'),\n",
       " ('perfectly', 'RB'),\n",
       " ('uniform', 'JJ'),\n",
       " ('seemed', 'VBD'),\n",
       " ('wall', 'NN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nltk.pos_tag(new_train.iloc[0]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not that bad...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "process\n",
      "however\n",
      "afford\n",
      "mean\n",
      "ascertain\n",
      "dimension\n",
      "dungeon\n",
      "i\n",
      "might\n",
      "make\n",
      "circuit\n",
      "return\n",
      "point\n",
      "whence\n",
      "i\n",
      "set\n",
      "without\n",
      "aware\n",
      "fact\n",
      "perfectly\n",
      "uniform\n",
      "seem\n",
      "wall\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    \n",
    "tokens_and_tags = nltk.pos_tag([t.lower() for t in new_train.iloc[0]['tokens']])\n",
    "\n",
    "for token, tag in tokens_and_tags:\n",
    "    print(lemmatizer.lemmatize(token, get_wordnet_pos(tag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seems good (to me)\n",
    "\n",
    "Let's change `prepare_df_for_LAD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id26305</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[this, process, however, afford, mean, ascerta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17569</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[it, never, occur, fumble, might, mere, mistake]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id11008</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[in, left, hand, gold, snuff, box, caper, hill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27763</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[how, lovely, spring, a, looked, windsor, terr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id12958</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[find, nothing, else, even, gold, superintende...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id22965</th>\n",
       "      <td>A youth passed in solitude, my best years spen...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[a, youth, pass, solitude, best, year, spend, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id09674</th>\n",
       "      <td>The astronomer, perhaps, at this point, took r...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[the, astronomer, perhaps, point, take, refuge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id13515</th>\n",
       "      <td>The surcingle hung in ribands from my body.</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[the, surcingle, hung, ribands, body]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id19322</th>\n",
       "      <td>I knew that you could not say to yourself 'ste...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[i, know, say, 'stereotomy', without, bring, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id00912</th>\n",
       "      <td>I confess that neither the structure of langua...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[i, confess, neither, structure, languages, co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text author  \\\n",
       "id                                                                  \n",
       "id26305  This process, however, afforded me no means of...    EAP   \n",
       "id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "id22965  A youth passed in solitude, my best years spen...    MWS   \n",
       "id09674  The astronomer, perhaps, at this point, took r...    EAP   \n",
       "id13515        The surcingle hung in ribands from my body.    EAP   \n",
       "id19322  I knew that you could not say to yourself 'ste...    EAP   \n",
       "id00912  I confess that neither the structure of langua...    MWS   \n",
       "\n",
       "                                                    tokens  \n",
       "id                                                          \n",
       "id26305  [this, process, however, afford, mean, ascerta...  \n",
       "id17569   [it, never, occur, fumble, might, mere, mistake]  \n",
       "id11008  [in, left, hand, gold, snuff, box, caper, hill...  \n",
       "id27763  [how, lovely, spring, a, looked, windsor, terr...  \n",
       "id12958  [find, nothing, else, even, gold, superintende...  \n",
       "id22965  [a, youth, pass, solitude, best, year, spend, ...  \n",
       "id09674  [the, astronomer, perhaps, point, take, refuge...  \n",
       "id13515              [the, surcingle, hung, ribands, body]  \n",
       "id19322  [i, know, say, 'stereotomy', without, bring, t...  \n",
       "id00912  [i, confess, neither, structure, languages, co...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "\n",
    "\n",
    "def prepare_df_for_LAD(df):\n",
    "    def tokenize(text):\n",
    "        return tokenizer.tokenize(text)\n",
    "    \n",
    "    def remove_stop_words(tokens):\n",
    "        english_stop_words = get_stop_words('en')\n",
    "\n",
    "        return [token for token in tokens if token not in english_stop_words]\n",
    "    \n",
    "    def lower(tokens):\n",
    "        return [token.lower() for token in tokens]\n",
    "    \n",
    "    def lemmatize(tokens):\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        tokens_and_tags = nltk.pos_tag(tokens)\n",
    "        \n",
    "        lemmas = []\n",
    "\n",
    "        for token, tag in tokens_and_tags:\n",
    "            lemma = lemmatizer.lemmatize(token, get_wordnet_pos(tag))\n",
    "            lemmas.append(lemma)\n",
    "            \n",
    "        return lemmas\n",
    "    \n",
    "    def prepare(text):\n",
    "        intiial_tokens = tokenize(text)\n",
    "        no_stop_words_tokens = remove_stop_words(intiial_tokens)\n",
    "        lowered_tokens = lower(no_stop_words_tokens)\n",
    "        lemmatized_tokens = lemmatize(lowered_tokens)\n",
    "        \n",
    "        return lemmatized_tokens\n",
    "\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    new_df['tokens'] = new_df.apply(lambda r: prepare(r['text']), axis=1)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "\n",
    "new_train = prepare_df_for_LAD(df=train)\n",
    "new_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's time for Latent Dirichlet Allocation\n",
    "\n",
    "Now, we need to convert our new tokens to a `bag-of-words`. This will be the \"dictionary\" of our LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_count_word = {\"features__ngram_range\": [(1,1), (1,2), (1,3)],\n",
    "                      \"features__analyzer\": ['word'],\n",
    "                      \"features__max_df\":[1.0, 0.9, 0.8, 0.7, 0.6, 0.5],\n",
    "                      \"features__min_df\":[2, 3, 5, 10],\n",
    "                      \"features__lowercase\": [False, True]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=5):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "def random_search():\n",
    "    params = {\n",
    "        'lda__learning_method': ['online'],\n",
    "        'lda__learning_offset': [20., 30., 40., 50., 60.]\n",
    "    }\n",
    "\n",
    "    params.update(params_count_word)\n",
    "    \n",
    "    # TODO:\n",
    "    # FeatureUnion: LatentDirichletAllocation + CountVectorizer -> LogisticRegresion / MultinomialNB\n",
    "\n",
    "#     pipeline = Pipeline([\n",
    "#         ('features', CountVectorizer()),\n",
    "#         ('lda', LatentDirichletAllocation())\n",
    "#     ])\n",
    "\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=params, \n",
    "                                       scoring='neg_log_loss',\n",
    "                                       n_iter=20, cv=3, n_jobs=4)\n",
    "\n",
    "    random_search.fit(new_train.text, new_train.author)\n",
    "    report(random_search.cv_results_)\n",
    "\n",
    "# random_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
