{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тук ще пробваме невронни мрежи, за да се справим с токсичните коментари\n",
    "\n",
    "Намерих един kernel в Kaggle, където използват Bidirection LSTM, за да получат доста добри резултати.\n",
    "\n",
    "За embeddings се ползва GloVe.\n",
    "\n",
    "## Защо LSTM?\n",
    "\n",
    "LSTM невронните мрежи са известни с това, че се справят доста добре в класификацията на текст.\n",
    "\n",
    "## Защо Bidirectional?\n",
    "\n",
    "Когато се използва BLSTM, в началото на input layer-a се тренират 2 LSTM мрежи вместо една - първата се тренира върху текста, а втората върху текста обърнат наобратно. Експерименти през последните години показват, че това увеличава значително performance-a на модели, които целят sequential classification. Ключовата прична за това е, че чрез тях мрежата знае за миналото и за бъдещето по едно и също време."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взимаме си данните първо, както направихме и в другия файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.csv.zip', index_col=['id'])\n",
    "test_data = pd.read_csv('./data/test.csv.zip', index_col=['id'])\n",
    "\n",
    "list_sentences_train = train_data[\"comment_text\"].fillna(\"empty\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_data[list_classes].values\n",
    "list_sentences_test = test_data[\"comment_text\"].fillna(\"empty\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Някои базови променливи за embedding-ите."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMEDDING_SIZE = 50 # how big is each word vector\n",
    "MAX_FEATURES = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_WORDS = 100 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартен начин на preprocessing, който се препоръчва от keras. Превръщаме коментарите в списък от индекси."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = pad_sequences(list_tokenized_train, maxlen=MAX_WORDS)\n",
    "X_test = pad_sequences(list_tokenized_test, maxlen=MAX_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ще напълним един речник, където пазим word -> vector репрезентациите."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
      "\n",
      "[ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n"
     ]
    }
   ],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "with open('./data/glove/glove.6B.50d.txt') as f:\n",
    "    glove = f.readlines()\n",
    "    print(glove[0])\n",
    "    embeddings_index= dict(get_coefs(*l.strip().split()) for l in glove)\n",
    "    \n",
    "print(embeddings_index['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега ще използваме речника, за да си направим embeddings матрицата. За думите, които нямаме, ще използваме random стойности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.020940498, 0.6441043)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean, emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(MAX_FEATURES, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMEDDING_SIZE))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_FEATURES:\n",
    "        continue\n",
    "    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преди да започнем\n",
    "\n",
    "Тук е времето да кажа, че е много важно да си четеш лекциите **внимателно**, за да не пускаш моделите си за 3ти път!!! Вчера тренирах със `sigmoid`, което е напълно грешно за моята задача, въпреки че постигах доста високи резултати. Тук е правилно да се използва `softmax`, тъй като моята задача е `multiclasss classification`. Със `sigmoid` имах успех от около 98.3. Сега ще видим какво ще излезе, очаквам по-слаб резултат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 630s 4ms/step - loss: 0.2220 - acc: 0.9642 - val_loss: 0.2193 - val_acc: 0.9634\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 623s 4ms/step - loss: 0.2181 - acc: 0.9648 - val_loss: 0.2184 - val_acc: 0.9649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc655c59e48>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = Input(shape=(MAX_WORDS,))\n",
    "x = Embedding(MAX_FEATURES, EMEDDING_SIZE, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"softmax\")(x)\n",
    "\n",
    "blstm = Model(inputs=inp, outputs=x)\n",
    "blstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "blstm.fit(X_train, y, batch_size=32, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Супер!\n",
    "\n",
    "Току-що направих първата си невронна мрежа с помощта на keras.\n",
    "\n",
    "\n",
    "### Сега ще пробвам малко модификации\n",
    "\n",
    "На една от последните лекции казахте, че GRU се справят добре, колкото LSTM, но с доста по-малко изчисления. Ще пробвам да използвам GRU вместо LSTM. Ще сложим още едно ниво с GRU, този път unidirectional. Слагам и Dropout, за да избягаме от overfitting. Ще увелича и броя на епохите."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 791s 6ms/step - loss: 0.2221 - acc: 0.9641 - val_loss: 0.2198 - val_acc: 0.9633\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 759s 5ms/step - loss: 0.2184 - acc: 0.9648 - val_loss: 0.2187 - val_acc: 0.9647\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 762s 5ms/step - loss: 0.2169 - acc: 0.9655 - val_loss: 0.2183 - val_acc: 0.9645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc6548cb4a8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "\n",
    "inp = Input(shape=(MAX_WORDS,))\n",
    "x = Embedding(MAX_FEATURES, EMEDDING_SIZE, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(GRU(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = GRU(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"softmax\")(x)\n",
    "\n",
    "gru = Model(inputs=inp, outputs=x)\n",
    "gru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "gru.fit(X_train, y, batch_size=32, epochs=3, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Както и предположих, GRU се справи добре както и LSTM-a. За 2 епохи постигна същия резултат. Наблюдава се подобрение при повече епохи.\n",
    "\n",
    "### Следващ вариант\n",
    "\n",
    "Следващата вариация, която ще използваме е да добавим конволюционна невронна мрежа след BLSTM-a. В [този paper](http://www.aclweb.org/anthology/C16-1329) се разказва за добри резултати при използването на тази комбинация за sequence classification. Ще пробвам и с малко повече епохи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/4\n",
      "143613/143613 [==============================] - 3866s 27ms/step - loss: 0.0642 - acc: 0.9785 - val_loss: 0.0519 - val_acc: 0.9812\n",
      "Epoch 2/4\n",
      "143613/143613 [==============================] - 3862s 27ms/step - loss: 0.0513 - acc: 0.9817 - val_loss: 0.0529 - val_acc: 0.9810\n",
      "Epoch 3/4\n",
      "143613/143613 [==============================] - 3848s 27ms/step - loss: 0.0463 - acc: 0.9827 - val_loss: 0.0527 - val_acc: 0.9820\n",
      "Epoch 4/4\n",
      "143613/143613 [==============================] - 3845s 27ms/step - loss: 0.0428 - acc: 0.9836 - val_loss: 0.0513 - val_acc: 0.9824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc66634a6a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "inp = Input(shape=(MAX_WORDS,))\n",
    "x = Embedding(MAX_FEATURES, EMEDDING_SIZE, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(300, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = Conv1D(100, 3, activation='relu', padding='same')(x)\n",
    "x = Conv1D(100, 3, activation='relu')(x)\n",
    "x = MaxPooling1D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(300, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(200, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(100, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y, batch_size=32, epochs=4, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В [примерите на keras](https://github.com/keras-team/keras/tree/ce4947cbaf380589a63def4cc6eb3e460c41254f/examples) намерих един, който прави класификация на текстове от IMDB. Вътре използват Conv1D + LSTM, като мен, но лагат конволюционната мрежа преди рекурентната. Потърсих и други примери  в интернет и там правят същото. Смятам, че горната мрежа не се справи много добре, защото съм разменил местата на мрежите.\n",
    "\n",
    "Ще опитаме да имплементираме [примера на keras](https://github.com/keras-team/keras/blob/ce4947cbaf380589a63def4cc6eb3e460c41254f/examples/imdb_cnn_lstm.py), като този път разположа мрежите по правилния начин. Те не ползват BLSTM, а просто LSTM, но аз ще се придържам към bidirectional варианта.\n",
    "\n",
    "Там имат само по 25 000 реда в сравнение с нашите ~160 000. Пише, че заради това стигат само 2 епохи. Аз ще пробвам с повече, заради по-големия обем данни."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/5\n",
      "143613/143613 [==============================] - 293s 2ms/step - loss: 0.2228 - acc: 0.9641 - val_loss: 0.2205 - val_acc: 0.9637\n",
      "Epoch 2/5\n",
      "143613/143613 [==============================] - 279s 2ms/step - loss: 0.2187 - acc: 0.9651 - val_loss: 0.2197 - val_acc: 0.9659\n",
      "Epoch 3/5\n",
      "143613/143613 [==============================] - 281s 2ms/step - loss: 0.2165 - acc: 0.9661 - val_loss: 0.2196 - val_acc: 0.9637\n",
      "Epoch 4/5\n",
      "143613/143613 [==============================] - 283s 2ms/step - loss: 0.2144 - acc: 0.9669 - val_loss: 0.2208 - val_acc: 0.9642\n",
      "Epoch 5/5\n",
      "143613/143613 [==============================] - 284s 2ms/step - loss: 0.2126 - acc: 0.9678 - val_loss: 0.2212 - val_acc: 0.9633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc64b187cf8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Activation\n",
    "\n",
    "FILTERS = 64\n",
    "KERNEL_SIZE = 5\n",
    "POOL_SIZE = 4\n",
    "\n",
    "conv_blstm = Sequential()\n",
    "conv_blstm.add(Embedding(MAX_FEATURES, EMEDDING_SIZE, input_length=MAX_WORDS))\n",
    "conv_blstm.add(Dropout(0.1))\n",
    "\n",
    "conv_blstm.add(Conv1D(FILTERS,\n",
    "                 KERNEL_SIZE,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "conv_blstm.add(MaxPooling1D(pool_size=POOL_SIZE))\n",
    "\n",
    "conv_blstm.add(Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))\n",
    "conv_blstm.add(GlobalMaxPool1D())\n",
    "\n",
    "conv_blstm.add(Dense(50))\n",
    "conv_blstm.add(Activation('relu'))\n",
    "conv_blstm.add(Dropout(0.1))\n",
    "conv_blstm.add(Dense(6))\n",
    "conv_blstm.add(Activation('softmax'))\n",
    "\n",
    "conv_blstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "conv_blstm.fit(X_train, y, batch_size=32, epochs=5, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интересно дали с увеличаване на епохите, моделът ще се стабилизира."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/10\n",
      "143613/143613 [==============================] - 281s 2ms/step - loss: 0.2108 - acc: 0.9684 - val_loss: 0.2231 - val_acc: 0.9632\n",
      "Epoch 2/10\n",
      "143613/143613 [==============================] - 283s 2ms/step - loss: 0.2094 - acc: 0.9690 - val_loss: 0.2258 - val_acc: 0.9633\n",
      "Epoch 3/10\n",
      "143613/143613 [==============================] - 283s 2ms/step - loss: 0.2082 - acc: 0.9695 - val_loss: 0.2257 - val_acc: 0.9618\n",
      "Epoch 4/10\n",
      "143613/143613 [==============================] - 281s 2ms/step - loss: 0.2076 - acc: 0.9696 - val_loss: 0.2274 - val_acc: 0.9623\n",
      "Epoch 5/10\n",
      "143613/143613 [==============================] - 282s 2ms/step - loss: 0.2068 - acc: 0.9700 - val_loss: 0.2267 - val_acc: 0.9628\n",
      "Epoch 6/10\n",
      "143613/143613 [==============================] - 284s 2ms/step - loss: 0.2063 - acc: 0.9702 - val_loss: 0.2322 - val_acc: 0.9624\n",
      "Epoch 7/10\n",
      "143613/143613 [==============================] - 283s 2ms/step - loss: 0.2057 - acc: 0.9704 - val_loss: 0.2331 - val_acc: 0.9634\n",
      "Epoch 8/10\n",
      "143613/143613 [==============================] - 282s 2ms/step - loss: 0.2053 - acc: 0.9706 - val_loss: 0.2286 - val_acc: 0.9638\n",
      "Epoch 9/10\n",
      "143613/143613 [==============================] - 282s 2ms/step - loss: 0.2049 - acc: 0.9708 - val_loss: 0.2300 - val_acc: 0.9631\n",
      "Epoch 10/10\n",
      "143613/143613 [==============================] - 284s 2ms/step - loss: 0.2045 - acc: 0.9709 - val_loss: 0.2333 - val_acc: 0.9631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc64da62ba8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_blstm.fit(X_train, y, batch_size=32, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постигнахме по-висока оценка, но за сметка на това validation accuracy-то ни падна. Това означава, че моделът ни по-скоро overfit-ва.\n",
    "\n",
    "Да видим как ще се справят моделите на тренировъчните данни."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidirectional LSTM\n",
      "153164/153164 [==============================] - 85s 553us/step\n",
      "Bidirectioanl GRU\n",
      "153164/153164 [==============================] - 125s 816us/step\n",
      "Convlution + BLSTM\n",
      "153164/153164 [==============================] - 48s 313us/step\n"
     ]
    }
   ],
   "source": [
    "print('Bidirectional LSTM')\n",
    "y_test_blstm = blstm.predict([X_test], batch_size=1024, verbose=1)\n",
    "print('Bidirectioanl GRU')\n",
    "y_test_gru = gru.predict([X_test], batch_size=1024, verbose=1)\n",
    "print('Convlution + BLSTM')\n",
    "y_test_conv_blstm = conv_blstm.predict([X_test], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample(y, suff):\n",
    "    sample_submission = pd.read_csv('./data/sample_submission.csv.zip')\n",
    "    sample_submission[list_classes] = y\n",
    "    sample_submission.to_csv(f'./submissions/submission_{suff}.csv', index=False)\n",
    "\n",
    "make_sample(y_test_blstm, 'blstm')\n",
    "make_sample(y_test_gru, 'gru')\n",
    "make_sample(y_test_conv_blstm, 'conv_blstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
