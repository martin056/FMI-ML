{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какво ще правя?\n",
    "\n",
    "Харесах си [едно състезание в kaggle](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). Става въпрос за класифициране на текст и по-точно на \"токсични коментари\".\n",
    "\n",
    "От описанието на състезанието, токсичните коментари имат няколко характеристики (toxic, severe_toxic, obscene, threat, insult, identity_hate), а целта накрая е да се даде оценка на характеристиките на всеки коментар. Коментарите са от статии в Wikipedia и са label-нати от доброволци.\n",
    "\n",
    "*П.П. Може да има и малко по-остра реч в тях.*\n",
    "\n",
    "\n",
    "# Да започваме същинската част\n",
    "\n",
    "\n",
    "Първо ще трябва да си направя някаквъ базов модел. Към момента в състезанието високите оценки са от сорта на `98.67`, така че гоним базов модел със сравнително висока оценка.\n",
    "\n",
    "За целта смятам да използвам някаква вариация на Naive Bayes - все пак ще работим с текст.\n",
    "\n",
    "Да видим все пак как изглежда данните."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>806fea2c9bdddf3b</th>\n",
       "      <td>Court records are by definition both accurate ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b66c16a90ac8f965</th>\n",
       "      <td>\"\\n\\n Recent page moves (see also: unresolved ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ebbf00e1ebe86f7e</th>\n",
       "      <td>douch bag \\n\\nThats you.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c39553a9b227b554</th>\n",
       "      <td>! And I'm not a sock puppet of Gogo Dodo But s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1feb0389279d02ce</th>\n",
       "      <td>I WAS NOT TROLLING, I WAS SUGGESTING THAT HE W...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9ec1390d1031ecc1</th>\n",
       "      <td>We have tons of research to back it up. Just l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c4ab8ff9d664b5a2</th>\n",
       "      <td>Throughout this season of Full House, Michelle...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9807839c92c15175</th>\n",
       "      <td>\"The Bulgarians were never tengriists, neither...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c325cb0060687864</th>\n",
       "      <td>\"\\n\\nUnblock: South Korean population update. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4af45c21f752133a</th>\n",
       "      <td>So, could you be more specific?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f04ed6debd0ca78c</th>\n",
       "      <td>\"Thank you for experimenting with  Wikipedia. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d7ceb24532aeb4ef</th>\n",
       "      <td>\"\\n\\nEDIT: I just finished trying to cleaup th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0b29c572320a674e</th>\n",
       "      <td>re: disruption \\n\\nplease, don't think me rude...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3fc558c7d8c2a8ba</th>\n",
       "      <td>wikipedia is a cold place to be.... maybe i sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63b0eca3a14a9336</th>\n",
       "      <td>Thanks Rossami, although I'm not sure that the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "806fea2c9bdddf3b  Court records are by definition both accurate ...      0   \n",
       "b66c16a90ac8f965  \"\\n\\n Recent page moves (see also: unresolved ...      0   \n",
       "ebbf00e1ebe86f7e                           douch bag \\n\\nThats you.      1   \n",
       "c39553a9b227b554  ! And I'm not a sock puppet of Gogo Dodo But s...      0   \n",
       "1feb0389279d02ce  I WAS NOT TROLLING, I WAS SUGGESTING THAT HE W...      1   \n",
       "9ec1390d1031ecc1  We have tons of research to back it up. Just l...      0   \n",
       "c4ab8ff9d664b5a2  Throughout this season of Full House, Michelle...      0   \n",
       "9807839c92c15175  \"The Bulgarians were never tengriists, neither...      0   \n",
       "c325cb0060687864  \"\\n\\nUnblock: South Korean population update. ...      0   \n",
       "4af45c21f752133a                    So, could you be more specific?      0   \n",
       "f04ed6debd0ca78c  \"Thank you for experimenting with  Wikipedia. ...      0   \n",
       "d7ceb24532aeb4ef  \"\\n\\nEDIT: I just finished trying to cleaup th...      0   \n",
       "0b29c572320a674e  re: disruption \\n\\nplease, don't think me rude...      0   \n",
       "3fc558c7d8c2a8ba  wikipedia is a cold place to be.... maybe i sh...      0   \n",
       "63b0eca3a14a9336  Thanks Rossami, although I'm not sure that the...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "id                                                                      \n",
       "806fea2c9bdddf3b             0        0       0       0              0  \n",
       "b66c16a90ac8f965             0        0       0       0              0  \n",
       "ebbf00e1ebe86f7e             0        0       0       0              0  \n",
       "c39553a9b227b554             0        0       0       0              0  \n",
       "1feb0389279d02ce             0        0       0       0              0  \n",
       "9ec1390d1031ecc1             0        0       0       0              0  \n",
       "c4ab8ff9d664b5a2             0        0       0       0              0  \n",
       "9807839c92c15175             0        0       0       0              0  \n",
       "c325cb0060687864             0        0       0       0              0  \n",
       "4af45c21f752133a             0        0       0       0              0  \n",
       "f04ed6debd0ca78c             0        0       0       0              0  \n",
       "d7ceb24532aeb4ef             0        0       0       0              0  \n",
       "0b29c572320a674e             0        0       0       0              0  \n",
       "3fc558c7d8c2a8ba             0        0       0       0              0  \n",
       "63b0eca3a14a9336             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('./data/train.csv.zip', index_col=['id'])\n",
    "test_data = pd.read_csv('./data/test.csv.zip', index_col=['id'])\n",
    "\n",
    "sample = train_data.sample(15)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ето и първия пример в цялото си изящество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Court records are by definition both accurate (people are sworn in) and significant (insignificant facts are stricken).  If this article is to include anything from the Eichenwald article, it must also contain any relevant information concerning the motivations and therefore the accuracy of Eichenwald's writing.  If the court records (which Eichenwald incidentally strove for months to keep sealed) are not considered significant, then everything in this article sourced from Eichenwald's exposé must be removed.\n"
     ]
    }
   ],
   "source": [
    "print(sample['comment_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514\n",
      "3511\n",
      "22\n",
      "65\n",
      "57\n",
      "120\n",
      "166\n",
      "1038\n",
      "179\n",
      "31\n",
      "270\n",
      "820\n",
      "116\n",
      "696\n",
      "447\n"
     ]
    }
   ],
   "source": [
    "for text in sample['comment_text']:\n",
    "    print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Както се вижда, дори и в тази малка извадка, дължината на текстовете варира много. Да видим как точно изглежда това на графика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    159571.000000\n",
       "mean        394.073221\n",
       "std         590.720282\n",
       "min           6.000000\n",
       "25%          96.000000\n",
       "50%         205.000000\n",
       "75%         435.000000\n",
       "max        5000.000000\n",
       "Name: comment_text, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = train_data['comment_text'].str.len()\n",
    "comments.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff510cdd278>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nboaGJKmboSFJ6mZoSJK6GRqSpG7/Hw6s8kXxJy56AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff50b79bba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вижда се, че почти всички коментари са с дължина около 0-1000, има малко до 2000 и след това са по-скоро аномалии (outlyer-и)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В малката извадка, която имаме, се вижда, че има коментар, който не е label-нат - всичките му колкони са 0. Ще направим нов label `unknown` и ще го сложим на подобни коментари."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.898321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "      <td>0.302226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate        unknown  \n",
       "count  159571.000000  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805       0.898321  \n",
       "std         0.216627       0.093420       0.302226  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       1.000000  \n",
       "50%         0.000000       0.000000       1.000000  \n",
       "75%         0.000000       0.000000       1.000000  \n",
       "max         1.000000       1.000000       1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_data['unknown'] = 1 - train_data[label_cols].max(axis=1)\n",
    "\n",
    "label_cols.append('unknown')\n",
    "\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff510b9c4a8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD9CAYAAACiLjDdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff510c16f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data[label_cols].mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преди да започнем с имплементацията на каквото и да било, ще си направя възможно най-базов модел. Ще използвам `CountVectorizer` + `MonomialNB` за него без да си играя да нагласям каквито и да било хиперпараметри, за да мога да добия някаква базова представа.\n",
    "\n",
    "За целта на нашите задачи, ще използваме `MultiOutputClassifier`, тъй като предвиждаме няколко класа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90067366, 0.9010779 , 0.90164191, 0.90098389, 0.90270728])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "\n",
    "pipeline = Pipeline([('vec', CountVectorizer()), ('clf', MultiOutputClassifier(MultinomialNB()))])\n",
    "cross_val_score(pipeline, train_data.comment_text, train_data[label_cols], cv=5, n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Not bad actually](https://media1.giphy.com/media/11wqqxRxm0uoY8/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В началото казах, че ще ползваме някаква форма на Naive Bayes\n",
    "\n",
    "В един от кърналите в състезанието се натъкнах на много интересен [paper](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf), работа на Sida Wang и Chris Maning. В него се говори за NBSVM, който ще използвам за базов модел.\n",
    "\n",
    "Накратко, те откриват релация между Monomial Naive Bayes и Support Vector Machine. След проведени експерименти, откриват, че MNB се справя доста добре с по-кратки текстове или т.нар. snippets, а SVM е по-добър в пълни по дължина ревюта.\n",
    "\n",
    "Това, като се замисля, е доста логично - ако подадеш на NB малки откази, където има псувни или думи, които ясно изразяват омраза, той лесно ще ги класифицира, като `toxic`, докато с по-пълни текстове ще има повече шум.\n",
    "\n",
    "Комбинацията NBSVM показва много добри резултати както със snippet-и, така и с пълни текстове, взимайки най-доброто от двата алгоритъма.\n",
    "\n",
    "Нещо, което правят, за да подобрят своите експерименти, е използването и на биграми, което смятам да опитам и аз.\n",
    "\n",
    "Според статията, с NBSVM е по-добре да се използва TFIDF, вместо count vectorizer или \"bag of words\" - винаги давало по-добри резултати. Обаче на мен горния резултат ми хареса достатъчно, така че засега ще пробвам и с двете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"'?\\b[0-9A-Za-z'-]+\\b'?\")  # matches numbers, words and words with apostrophes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Това горното очевидно си го копирах. Ето какво прави:\n",
    "\n",
    ">re.compile(f'...') compiles the expression([{string.punctuation}...]) into pattern objects (say, for later pattern matching or string substitutions) while the f'...' prefix causes the formatted string literal (f-string) {string.punctuation} to be evaluated at run-time. Note that string.punctuation is a readymade-string of common punctuation marks.\n",
    "\n",
    ">Later re_tok.sub(r' \\1 ', s) finds all the resulting matching-punctuations and adds a prefix and suffix of white-space to those matching patterns. Lastly, split() call tokenizes resulting string into an array of individual words and punctuation marks.\n",
    "\n",
    "Ето пример за нагледно обяснение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User',\n",
       " 'Rohedin',\n",
       " 're',\n",
       " 'your',\n",
       " 'warning',\n",
       " 'Uh',\n",
       " 'you',\n",
       " 'sure',\n",
       " 'about',\n",
       " 'that',\n",
       " 'I',\n",
       " 'see',\n",
       " 'two',\n",
       " 'edits',\n",
       " 'here',\n",
       " 'and',\n",
       " 'here',\n",
       " 'Looks',\n",
       " 'OK',\n",
       " 'to',\n",
       " 'me',\n",
       " 'or',\n",
       " 'am',\n",
       " 'I',\n",
       " 'missing',\n",
       " 'something',\n",
       " 'Cheers',\n",
       " 'TFOWR']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = tokenizer.tokenize(sample['comment_text'][0])\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега да махнем стоп думите."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User',\n",
       " 'Rohedin',\n",
       " 'warning',\n",
       " 'Uh',\n",
       " 'sure',\n",
       " 'I',\n",
       " 'see',\n",
       " 'two',\n",
       " 'edits',\n",
       " 'Looks',\n",
       " 'OK',\n",
       " 'I',\n",
       " 'missing',\n",
       " 'something',\n",
       " 'Cheers',\n",
       " 'TFOWR']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    english_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    return [token for token in tokens if token not in english_stop_words]\n",
    "\n",
    "remove_stop_words(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добре, близо сме. Ще изкарам една функция, за да тестваме дали имаме желания ефект. Вместо да използвам функцията, ще използваме аргументите на `CountVectorizer` и `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User',\n",
       " 'Rohedin',\n",
       " 'warning',\n",
       " 'Uh',\n",
       " 'sure',\n",
       " 'I',\n",
       " 'see',\n",
       " 'two',\n",
       " 'edits',\n",
       " 'Looks',\n",
       " 'OK',\n",
       " 'I',\n",
       " 'missing',\n",
       " 'something',\n",
       " 'Cheers',\n",
       " 'TFOWR']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def handle_tokenization(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    tokens_without_stop_words = remove_stop_words(tokens)\n",
    "    \n",
    "    return tokens_without_stop_words\n",
    "\n",
    "\n",
    "handle_tokenization(sample['comment_text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готови сме! Сега е време да си направим матрицата с думите. Ще използвам униграми и биграми, както пишеше в paper-a. За класификатори ще се пробвам с:\n",
    "- Мultinomial NB\n",
    "- LinearSVC\n",
    "- SVC\n",
    "- RandomForest\n",
    "\n",
    "Ще сравня резултатите от тях с NBSVM, който ще напишем сега."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "class NBSVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0, dual=False, n_jobs=1):\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict_proba(x.multiply(self._r))\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=5):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понеже чаках над 2 часа с RandomForest, ще го пусна само за 10 дървета, за да видя колко време ще ми отнеме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.89613035  0.89875917  0.89825782  0.89882183  0.89850849]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1475.0570828914642"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "\n",
    "def try_random_forest():\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    pipeline = Pipeline([('vec', CountVectorizer()), ('clf', MultiOutputClassifier(RandomForestClassifier()))])\n",
    "    print(cross_val_score(pipeline, train_data.comment_text, train_data[label_cols], cv=5, n_jobs=4))\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    return end - start\n",
    "\n",
    "# try_random_forest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```[ 0.89613035  0.89875917  0.89825782  0.89882183  0.89850849]\n",
    "Out[18]:\n",
    "1475.0570828914642```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Това е прекалено много и нямам време в момента да го чакам. Предполагам, че дори с малко увеличение на дурветата, ще даде по-добри резултати, но поради ред обстоятелства ще го оставя за по-късно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer + MultinomialNB\n",
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=4)]: Done  55 out of  55 | elapsed: 15.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.908 (std: 0.001)\n",
      "Parameters: {'clf__estimator__alpha': 0.50000000000000011, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__stop_words': 'english', 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\"}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.907 (std: 0.001)\n",
      "Parameters: {'clf__estimator__alpha': 0.70000000000000007, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__stop_words': 'english', 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\"}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.907 (std: 0.001)\n",
      "Parameters: {'clf__estimator__alpha': 0.30000000000000004, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__stop_words': 'english', 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\"}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.907 (std: 0.001)\n",
      "Parameters: {'clf__estimator__alpha': 0.90000000000000013, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__stop_words': 'english', 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\"}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation score: 0.906 (std: 0.001)\n",
      "Parameters: {'clf__estimator__alpha': 1.1000000000000003, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__stop_words': 'english', 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\"}\n",
      "\n",
      "CountVectorizer + LinearSVC\n",
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 55.8min\n",
      "[Parallel(n_jobs=4)]: Done  55 out of  55 | elapsed: 77.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.915 (std: 0.001)\n",
      "Parameters: {'clf__estimator__C': 0.10000000000000001, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__stop_words': 'english', 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\"}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.914 (std: 0.002)\n",
      "Parameters: {'clf__estimator__C': 0.30000000000000004, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__stop_words': 'english', 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\"}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.913 (std: 0.002)\n",
      "Parameters: {'clf__estimator__C': 0.50000000000000011, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__stop_words': 'english', 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\"}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.913 (std: 0.002)\n",
      "Parameters: {'clf__estimator__C': 0.70000000000000007, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__stop_words': 'english', 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\"}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation score: 0.912 (std: 0.002)\n",
      "Parameters: {'clf__estimator__C': 0.90000000000000013, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__stop_words': 'english', 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\"}\n",
      "\n",
      "CountVectorizer + SVC\n",
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b5218c02c38e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         )\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/machine-learning/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    636\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    637\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 638\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/machine-learning/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/machine-learning/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "\n",
    "count_vec_params = {\n",
    "    'vec__ngram_range': [(1,2)],\n",
    "    'vec__lowercase': [True],\n",
    "    'vec__analyzer': ['word'],\n",
    "    'vec__token_pattern': [r\"'?\\b[0-9A-Za-z'-]+\\b'?\"],\n",
    "    'vec__stop_words': ['english'],\n",
    "}\n",
    "tfidf_params = count_vec_params.copy().update({\n",
    "    'vec__use_idf': [True],\n",
    "    'vec__smooth_idf': [True],\n",
    "    'vec__sublinear_tf': [True]\n",
    "})\n",
    "\n",
    "vec_params = {\n",
    "    CountVectorizer: count_vec_params,\n",
    "    TfidfVectorizer: tfidf_params\n",
    "}\n",
    "\n",
    "hyper_parameter_values = list(np.arange(0.1, 1.5, 0.2)) + list(range(2, 10, 2))\n",
    "\n",
    "clf_params = {\n",
    "    MultinomialNB: {'clf__estimator__alpha': hyper_parameter_values},\n",
    "    LinearSVC: {'clf__estimator__C': hyper_parameter_values},\n",
    "    SVC: {'clf__estimator__C': hyper_parameter_values}\n",
    "}\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for vec in vec_params.keys():\n",
    "    for clf in clf_params.keys():\n",
    "        print(f'{vec.__name__} + {clf.__name__}')\n",
    "        params = vec_params[vec].copy()\n",
    "        params.update(clf_params[clf])\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('vec', vec()),\n",
    "            ('clf', MultiOutputClassifier(clf()))\n",
    "        ])\n",
    "\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid=params,\n",
    "            cv=5,\n",
    "            n_jobs=4,\n",
    "            verbose=1\n",
    "        )\n",
    "        grid_search.fit(train_data.comment_text, train_data[label_cols])\n",
    "        report(grid_search.cv_results_)\n",
    "        \n",
    "        results.append((grid_search.best_score_, grid_search.best_params_))\n",
    "        \n",
    "sorted(results, reverse=True, key=lambda res: res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` + `SVC` не намери решение повече 8 часа... Засега ще се спра на горните резултати. Ще изпробваме и какво ще се получи, с TFIDF. Според статията, трябва да имаме подобрение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.904 (std: 0.001)\n",
      "Parameters: {'clf__estimator__alpha': 0.1, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\", 'vec__use_idf': True}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.900 (std: 0.001)\n",
      "Parameters: {'clf__estimator__alpha': 0.30000000000000004, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\", 'vec__use_idf': True}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.899 (std: 0.001)\n",
      "Parameters: {'clf__estimator__alpha': 0.5000000000000001, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\", 'vec__use_idf': True}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.899 (std: 0.001)\n",
      "Parameters: {'clf__estimator__alpha': 0.7000000000000001, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\", 'vec__use_idf': True}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation score: 0.899 (std: 0.001)\n",
      "Parameters: {'clf__estimator__alpha': 0.9000000000000001, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\", 'vec__use_idf': True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "\n",
    "# pipeline = Pipeline([('vec', CountVectorizer()), ('clf', MultiOutputClassifier(MultinomialNB()))])\n",
    "# print(cross_val_score(pipeline, train_data.comment_text, train_data[label_cols], cv=5, n_jobs=4))\n",
    "\n",
    "hyper_parameter_values = list(np.arange(0.1, 1.5, 0.2)) + list(range(2, 10, 2))\n",
    "\n",
    "params = {\n",
    "    'vec__ngram_range': [(1,2)],\n",
    "    'vec__lowercase': [True],\n",
    "    'vec__analyzer': ['word'],\n",
    "    'vec__token_pattern': [r\"'?\\b[0-9A-Za-z'-]+\\b'?\"],\n",
    "    'vec__stop_words': ['english'],\n",
    "    'vec__use_idf': [True],\n",
    "    'vec__smooth_idf': [True],\n",
    "    'vec__sublinear_tf': [True]\n",
    "}\n",
    "\n",
    "params.update({'clf__estimator__alpha': hyper_parameter_values})\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vec', TfidfVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(MultinomialNB()))\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=params,\n",
    "    cv=5,\n",
    "    n_jobs=4,\n",
    ")\n",
    "grid_search.fit(train_data.comment_text, train_data[label_cols])\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тука виждам, че имам значително подобрение към 0.1, което ми се явява граница. Прабвах и с по-малки стойности (0.01 и 0.001), но нямаше значително подобрение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 16.5min\n",
      "[Parallel(n_jobs=4)]: Done  55 out of  55 | elapsed: 24.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.920 (std: 0.001)\n",
      "Parameters: {'clf__estimator__C': 0.30000000000000004, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\", 'vec__use_idf': True}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.920 (std: 0.001)\n",
      "Parameters: {'clf__estimator__C': 0.50000000000000011, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\", 'vec__use_idf': True}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.919 (std: 0.001)\n",
      "Parameters: {'clf__estimator__C': 0.70000000000000007, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\", 'vec__use_idf': True}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.919 (std: 0.001)\n",
      "Parameters: {'clf__estimator__C': 0.90000000000000013, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\", 'vec__use_idf': True}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation score: 0.918 (std: 0.001)\n",
      "Parameters: {'clf__estimator__C': 1.1000000000000003, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\", 'vec__use_idf': True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "params = {\n",
    "    'vec__ngram_range': [(1,2)],\n",
    "    'vec__lowercase': [True],\n",
    "    'vec__analyzer': ['word'],\n",
    "    'vec__token_pattern': [r\"'?\\b[0-9A-Za-z'-]+\\b'?\"],\n",
    "    'vec__stop_words': ['english'],\n",
    "    'vec__use_idf': [True],\n",
    "    'vec__smooth_idf': [True],\n",
    "    'vec__sublinear_tf': [True]\n",
    "}\n",
    "\n",
    "hyper_parameter_values = list(np.arange(0.1, 1.5, 0.2)) + list(range(2, 10, 2))\n",
    "\n",
    "params.update({'clf__estimator__C': hyper_parameter_values})\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vec', TfidfVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(LinearSVC()))\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=params,\n",
    "    cv=5,\n",
    "    n_jobs=4,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(train_data.comment_text, train_data[label_cols])\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Естествено, с последния вариант имаме победител. Докато чаках резултатите, се сетих, че не взимам под внимание нещо, което може да подобри модела ми. regex-a, който използваме за токенизация, в моменра хваща само думи (и такива с апострофи). Когато хората пишат токсични коментари, те често използват **последователности от препинателни знаци**. Например `.........`, `!?!?!?!!` или различни емотиконки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanished article question \n",
      "\n",
      "hi! i'm new here and i wanna ask you a question, there was a long article here yesterday Kinkeshi but i checked it today and the article has just disappeared!! do you know what happened to it? thanks! )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vanished',\n",
       " 'article',\n",
       " 'question',\n",
       " 'hi',\n",
       " '!',\n",
       " \"i'm\",\n",
       " 'new',\n",
       " 'here',\n",
       " 'and',\n",
       " 'i',\n",
       " 'wanna',\n",
       " 'ask',\n",
       " 'you',\n",
       " 'a',\n",
       " 'question',\n",
       " ',',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'long',\n",
       " 'article',\n",
       " 'here',\n",
       " 'yesterday',\n",
       " 'Kinkeshi',\n",
       " 'but',\n",
       " 'i',\n",
       " 'checked',\n",
       " 'it',\n",
       " 'today',\n",
       " 'and',\n",
       " 'the',\n",
       " 'article',\n",
       " 'has',\n",
       " 'just',\n",
       " 'disappeared',\n",
       " '!!',\n",
       " 'do',\n",
       " 'you',\n",
       " 'know',\n",
       " 'what',\n",
       " 'happened',\n",
       " 'to',\n",
       " 'it',\n",
       " '?',\n",
       " 'thanks',\n",
       " '!',\n",
       " ')']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "\n",
    "tokenizer = RegexpTokenizer(rf\"'?\\b[0-9A-Za-z'-]+\\b'?|[{string.punctuation}]+\")\n",
    "test_str = train_data.sample(1)['comment_text'][0]\n",
    "print(test_str)\n",
    "tokenizer.tokenize(test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Перфектно!** Ще пробвам последния вариант, за да видим дали ще имаме по-добър резултат.\n",
    "\n",
    "*f-strings <3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 21.2min\n",
      "[Parallel(n_jobs=4)]: Done  55 out of  55 | elapsed: 30.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.920 (std: 0.001)\n",
      "Parameters: {'clf__estimator__C': 0.30000000000000004, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': '\\'?\\\\b[0-9A-Za-z\\'-]+\\\\b\\'?|[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', 'vec__use_idf': True}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.920 (std: 0.001)\n",
      "Parameters: {'clf__estimator__C': 0.50000000000000011, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': '\\'?\\\\b[0-9A-Za-z\\'-]+\\\\b\\'?|[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', 'vec__use_idf': True}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.920 (std: 0.001)\n",
      "Parameters: {'clf__estimator__C': 0.70000000000000007, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': '\\'?\\\\b[0-9A-Za-z\\'-]+\\\\b\\'?|[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', 'vec__use_idf': True}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.919 (std: 0.001)\n",
      "Parameters: {'clf__estimator__C': 0.90000000000000013, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': '\\'?\\\\b[0-9A-Za-z\\'-]+\\\\b\\'?|[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', 'vec__use_idf': True}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation score: 0.919 (std: 0.001)\n",
      "Parameters: {'clf__estimator__C': 1.1000000000000003, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': '\\'?\\\\b[0-9A-Za-z\\'-]+\\\\b\\'?|[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+', 'vec__use_idf': True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "params = {\n",
    "    'vec__ngram_range': [(1,2)],\n",
    "    'vec__lowercase': [True],\n",
    "    'vec__analyzer': ['word'],\n",
    "    'vec__token_pattern': [rf\"'?\\b[0-9A-Za-z'-]+\\b'?|[{string.punctuation}]+\"],\n",
    "    'vec__stop_words': ['english'],\n",
    "    'vec__use_idf': [True],\n",
    "    'vec__smooth_idf': [True],\n",
    "    'vec__sublinear_tf': [True]\n",
    "}\n",
    "\n",
    "hyper_parameter_values = list(np.arange(0.1, 1.5, 0.2)) + list(range(2, 10, 2))\n",
    "\n",
    "params.update({'clf__estimator__C': hyper_parameter_values})\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vec', TfidfVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(LinearSVC()))\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=params,\n",
    "    cv=5,\n",
    "    n_jobs=4,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(train_data.comment_text, train_data[label_cols])\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добре, новият регулярен израз е доста готин, но не ни помогна особено много в този случай, а и ни забави, което е ключово за оставащото ми време. Дойде време най-после да изпробвам NBSVM. Ще го пусна с TFIDF и да видим какво ще стане."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.91944227  0.92047377  0.9214138   0.92100645  0.92119446]\n",
      "108.09142756462097\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "pipeline = Pipeline([('vec', TfidfVectorizer()), ('clf', MultiOutputClassifier(NBSVMClassifier()))])\n",
    "print(cross_val_score(pipeline, train_data.comment_text, train_data[label_cols], cv=5, n_jobs=4))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 46.4min\n",
      "[Parallel(n_jobs=4)]: Done  55 out of  55 | elapsed: 62.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.920 (std: 0.001)\n",
      "Parameters: {'clf__estimator__C': 0.90000000000000013, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\", 'vec__use_idf': True}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.919 (std: 0.001)\n",
      "Parameters: {'clf__estimator__C': 0.70000000000000007, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\", 'vec__use_idf': True}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.919 (std: 0.001)\n",
      "Parameters: {'clf__estimator__C': 0.50000000000000011, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\", 'vec__use_idf': True}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.919 (std: 0.001)\n",
      "Parameters: {'clf__estimator__C': 1.1000000000000003, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\", 'vec__use_idf': True}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean validation score: 0.919 (std: 0.001)\n",
      "Parameters: {'clf__estimator__C': 1.3000000000000003, 'vec__analyzer': 'word', 'vec__lowercase': True, 'vec__ngram_range': (1, 2), 'vec__smooth_idf': True, 'vec__stop_words': 'english', 'vec__sublinear_tf': True, 'vec__token_pattern': \"'?\\\\b[0-9A-Za-z'-]+\\\\b'?\", 'vec__use_idf': True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "params = {\n",
    "    'vec__ngram_range': [(1,2)],\n",
    "    'vec__lowercase': [True],\n",
    "    'vec__analyzer': ['word'],\n",
    "    'vec__token_pattern': [r\"'?\\b[0-9A-Za-z'-]+\\b'?\"],\n",
    "    'vec__stop_words': ['english'],\n",
    "    'vec__use_idf': [True],\n",
    "    'vec__smooth_idf': [True],\n",
    "    'vec__sublinear_tf': [True]\n",
    "}\n",
    "\n",
    "hyper_parameter_values = list(np.arange(0.1, 1.5, 0.2)) + list(range(2, 10, 2))\n",
    "\n",
    "params.update({'clf__estimator__C': hyper_parameter_values})\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vec', TfidfVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(NBSVMClassifier()))\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid=params,\n",
    "    cv=5,\n",
    "    n_jobs=4,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(train_data.comment_text, train_data[label_cols])\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За жалост този модел не се справи по-добре от обикновената линейна SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Да пробваме какво ще излезе и ако използваме невронна мрежа\n",
    "\n",
    "За целта ще пиша в нов notebook, тък като този стана прекалено дълъг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
